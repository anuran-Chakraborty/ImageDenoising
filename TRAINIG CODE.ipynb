{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running for noisy imag and gt image both in grayscale\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from torchvision import utils\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training import , 70 pictures per folder from 00 to 69\n",
    "\n",
    "img_size=256\n",
    "torch.cuda.empty_cache()\n",
    "img_noisy_tens=torch.empty([1470,3,img_size,img_size])  #Initializing tensor for noisy image\n",
    "\n",
    "img_mask_tens=torch.empty([1470,3,img_size,img_size])   #Initializing tensor for grayscale gt\n",
    "\n",
    "char_array=['agricultural','airplane','baseballdiamond','beach','buildings','chaparral','denseresidential','forest','freeway','golfcourse','harbor','intersection','mediumresidential','mobilehomepark','overpass','parkinglot','river','runway','sparseresidential','storagetanks','tenniscourt']\n",
    "\n",
    "for k in range(21):\n",
    "    for i in range(7):\n",
    "        for j in range(10):\n",
    "            n=cv2.imread('D:\\\\UCMerced_LandUse\\\\Img_noisy\\\\%s%d%d.tif' % (char_array[k],i,j))\n",
    "            n=np.resize(n,(256,256,3))\n",
    "            img_noisy_tens[70*k+(10*i+j)]=transforms.ToTensor()(n)\n",
    "            m=cv2.imread('D:\\\\UCMerced_LandUse\\\\Img_grayscale\\\\%s%d%d.tif' % (char_array[k],i,j))\n",
    "            m=np.resize(m,(256,256,3))\n",
    "            img_mask_tens[70*k+(10*i+j)]=transforms.ToTensor()(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying Noisy Grayscale image\n",
    "to_pil=transforms.ToPILImage()\n",
    "imgg=to_pil(img_noisy_tens[1469])\n",
    "imgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairing Noisy and Gt Image together then feeding to the dataloader\n",
    "\n",
    "batch_size=2\n",
    "img_merged_tens=torch.empty([1470,2,3,img_size,img_size])\n",
    "img_merged_tens[:,0,:,:,:]=img_noisy_tens\n",
    "img_merged_tens[:,1,:,:,:]=img_mask_tens\n",
    "\n",
    "#img_merged_tens contains masked and noisy image together in proper order\n",
    "\n",
    "\n",
    "img_dl=DataLoader(img_merged_tens,batch_size,shuffle=True)\n",
    "img_batch_sample=next(iter(img_dl))\n",
    "\n",
    "#Dataloader of batch size 8 is created and has been shuffled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying noisy and gt grayscale image side by side \n",
    "\n",
    "grid_img=utils.make_grid(img_batch_sample[1],nrow=2)\n",
    "grid_img.shape\n",
    "plt.imshow(grid_img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.conv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#from .unet_parts import *\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.out_channels = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, self.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading saved models if any\n",
    "\n",
    "import os\n",
    "\n",
    "device=torch.device(\"cuda:0\")\n",
    "unet=UNet(4,3)\n",
    "resume=True\n",
    "PATH='D:\\\\UCMerced_LandUse\\\\models'\n",
    "if(resume):\n",
    "    # Load the model\n",
    "    unet.load_state_dict(torch.load(os.path.join(PATH,'model_latest')))\n",
    "unet=unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainig Block \n",
    "\n",
    "loss=nn.BCELoss()\n",
    "opt=torch.optim.Adam(unet.parameters(),lr=0.001)\n",
    "epoch=500\n",
    "for epoc in range(epoch):\n",
    "    itr=0\n",
    "    loss_sum=0\n",
    "    for i in img_dl:\n",
    "        input_batch  = i[:,0,:,:,:].to(device)\n",
    "        target_batch = i[:,1,:,:,:].to(device)\n",
    "        out=unet(input_batch)\n",
    "        loss_out=loss(out,target_batch)\n",
    "        loss_sum+=loss_out\n",
    "        loss_out.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        itr+=1\n",
    "        print('loss = {} , epoch = {}, iteration ={}'.format(loss_out,epoc,itr))\n",
    "        if(itr%5==0):\n",
    "            torch.save(unet.state_dict(), os.path.join(PATH,'model_latest'))\n",
    "            torch.save(unet.state_dict(), os.path.join(PATH,'model_'+str(itr)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
